{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea5a41c-7062-4d6a-a7dd-ec9a8294f448",
   "metadata": {},
   "source": [
    "#### Name: Reda Mohsen Reda\n",
    "#### ID: 18P5141\n",
    "#### Course: Deep Learning\n",
    "#### Topic: Hand Gesture Recognition Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f5600-0f8d-4d55-800f-069e26ca660f",
   "metadata": {},
   "source": [
    "# Instal Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7f8db1-1735-4ed3-8c78-023e66822716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow opencv-python matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f84b55-af4e-4662-84fd-bc907a8a74d0",
   "metadata": {},
   "source": [
    "# Import Libraries and Setup GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60b860-f229-4dd8-9180-9caa17e0f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df39f34-c921-4108-8f47-99cc01e2ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f38eed-98b2-4c53-a253-7578d36568bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout, Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d783cb5-9b3d-40bc-a8ec-d8fddabdb8f0",
   "metadata": {},
   "source": [
    "# About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5b475-20f1-4240-954e-7b04dd00b2f2",
   "metadata": {},
   "source": [
    "[`hand-gesture-recognition-dataset`](https://www.kaggle.com/datasets/aryarishabh/hand-gesture-recognition-dataset?resource=download) This dataset contains total 24000 images of 20 different gestures. For training purpose, there are 900 images in each directory and for testing purpose there are 300 images in each directory.\n",
    "\n",
    "![Full dataset](Images/dataset-cover.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da095c43-9c74-4536-b688-4fc00f19bc1c",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74423a80-7b0f-418b-9613-e8a1cc41c29c",
   "metadata": {},
   "source": [
    " Loading the data into training dataset and testing dataset using tensorflow [`utils`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2d1a9-776a-43cf-afce-156303a9dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 50\n",
    "img_width = 50\n",
    "class_names = ['0','1','2','3','4','5','6','7','8','9',\n",
    "               '10','11','12','13','14','15','16','17',\n",
    "               '18','19']\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    'dataset/train',\n",
    "    labels='inferred',\n",
    "    # label_mode='categorical',\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale',\n",
    "    class_names=class_names,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    seed=1,\n",
    "    validation_split=0.33335,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    'dataset/train',\n",
    "    labels='inferred',\n",
    "    # label_mode='categorical',\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale',\n",
    "    class_names=class_names,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    seed=1,\n",
    "    validation_split=0.33335,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    'dataset/test',\n",
    "    labels='inferred',\n",
    "    # label_mode='categorical',\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale',\n",
    "    class_names=class_names,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545940a-7e63-42ab-a414-0b51f149fd44",
   "metadata": {},
   "source": [
    "if label_mode is categorical, the labels are a float32 tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.\n",
    "if color_mode is grayscale, there's 1 channel in the image tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412b584-27bc-47a8-8b5d-38945b62d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07b036-37df-420d-9581-76420c45571c",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed33cc8-a9fd-4f4b-b558-b918e2b618a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c92bf-2e94-44e3-a08e-c32282f2c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "normalized_valid_dataset = valid_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_train_dataset))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406e7d7-72d5-4789-9c88-13712df42f6c",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84570cc1-6591-41b4-8391-47d3faf85de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Images Shape', image_batch.shape)\n",
    "print('Labels Shape', labels_batch.shape)\n",
    "plt.imshow(image_batch[0],cmap='gray')\n",
    "plt.title(class_names[labels_batch[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc2e90-30e5-49c4-937a-3eac3d0facf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_dataset.take(1):\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image_batch[i],cmap='gray')\n",
    "    # plt.title(class_names[labels_batch[i]])\n",
    "    plt.title(labels_batch[i])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f25b59-17df-43da-b4d5-b6a7b32a304d",
   "metadata": {},
   "source": [
    "# Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4b854-ac44-415b-872c-155d8442644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = normalized_train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_dataset = normalized_valid_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c06be-7d77-42d6-aaf8-b4567d2165a4",
   "metadata": {},
   "source": [
    "# Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3164543-7356-4ff3-bb24-88a0d4f91d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Layer 1\n",
    "model.add(Conv2D(filters=6, input_shape=(50,50,1), kernel_size=5, strides=1, activation='relu',padding=\"same\"))\n",
    "# Layer 2\n",
    "model.add(AveragePooling2D(strides=2))\n",
    "# Layer 3\n",
    "model.add(Conv2D(filters=16 ,kernel_size=5, strides=1, activation='relu'))\n",
    "# Layer 4\n",
    "model.add(AveragePooling2D(strides=2))\n",
    "# Layer 5\n",
    "model.add(Conv2D(filters=120,kernel_size=5,strides=1, activation='relu'))\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "# Layer 6\n",
    "model.add(Dense(120, activation='relu'))\n",
    "# Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Layer 7\n",
    "model.add(Dense(84, activation='relu'))\n",
    "# Output Layer\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f5e1e-1b09-4d76-8b28-55bc16ce4e75",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c6bb1-1ce0-4786-bd95-39efe0fd8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1322b84-7d8e-404a-b7e4-db0b2b004f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# class CustomCallback(keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs=None):\n",
    "#         self.now= time.time()\n",
    "#     def on_train_end(self,totaltime, logs=None):\n",
    "#         later=time.time()\n",
    "#         duration=later-self.now \n",
    "#         totaltime = duration\n",
    "#         print('Total Time is: ', totaltime, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3517bb-8038-4747-b2cd-d2eeca713018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "# hist = model.fit(train_dataset, validation_data=valid_dataset, epochs=epochs, batch_size=batch_size, callbacks=[CustomCallback(),tensorboard_callback])\n",
    "epochs=50\n",
    "hist = model.fit(train_dataset, validation_data=valid_dataset, epochs=epochs, batch_size=batch_size, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d966f-ffd0-4691-9e7a-cb4acb5475a4",
   "metadata": {},
   "source": [
    "# Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bf46e-dd73-4823-93ee-cd9e412d0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6799031-d1c6-4733-927e-a86b6ba2b48e",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a173b-f48e-4a64-9ed5-9530534ec06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nEvaluating:')\n",
    "(test_loss, test_accuracy) = model.evaluate(test_dataset)\n",
    "print(f'\\nTest accuracy: {test_accuracy * 100:>0.1f}%, Test loss: {test_loss:>8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0133b92-6ee8-472a-b42b-e83d13a1b60d",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fcbed2-82ef-4286-a1e7-4b1d601429fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "# # Retrieve a batch of images from the test set\n",
    "# image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "# predictions = model.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "# # Apply a sigmoid since our model returns logits\n",
    "# # predictions = tf.nn.sigmoid(predictions)\n",
    "# # predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "# print('Predictions:\\n', predictions.numpy())\n",
    "# print('Labels:\\n', label_batch)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(9):\n",
    "#   ax = plt.subplot(3, 3, i + 1)\n",
    "#   plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "#   plt.title(class_names[predictions[i]])\n",
    "#   plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e9bd3-5e3f-485a-a4ff-2c597bc6fa61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b357c2d5-5327-4426-80ad-c682884fbacd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e572651-f17a-4cf3-afaa-500ca6960704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e351d7d-48a5-4118-a52a-071079c99e0c",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c54de-7472-49d0-b083-451ba88a2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('outputs/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a254d0c-b21c-4a36-b90c-be18d6588e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_batch, labels_batch = next(iter(test_dataset))\n",
    "# fig = plt.figure(figsize = (20,20))\n",
    "# counter = 1\n",
    "# for image in image_batch[0:9]:\n",
    "#     fig.add_subplot(3,3,counter)\n",
    "#     pred = np.argmax(model.predict(image))\n",
    "#     plt.imshow(image,cmap=\"gray\")\n",
    "#     plt.title(\"Repd\"+str(pred)+\"True\"+str(label))\n",
    "#     counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09942943-ec58-430c-8707-9cce7fa61049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Label of image => test.label() , predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d04ae8-cd75-478f-86e7-b3ac4ea2e3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
